{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0c0fa8-585d-4033-b279-90e55cf2652d",
   "metadata": {},
   "source": [
    "# Cryosphere model Comparison tool (CmCt) Dynamic Thickness Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8788c4-1306-4c5d-a305-43ab8e805dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import cartopy.crs as ccrs\n",
    "import time\n",
    "from datetime import datetime\n",
    "import netCDF4\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502e670-dcae-4f12-83fa-498094c01c75",
   "metadata": {},
   "source": [
    "## About This Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48855a8e-bf67-49f1-905b-7f037e9ddeed",
   "metadata": {},
   "source": [
    "The CmCt Dynamic Thickness Tool compares the user's uploaded ice sheet model(s) to dynamic thickness data from IMAU, GSFC, and/or GEMB observation data. This observation data is a rectangular grid in UTM zone 24N coordinates but the model is expected as a rectangular grid in ISMIP6 polar-stereographic coordinates (see the Input Data Requirements section). The model data is bilinearly interpolated to observation space and the residual is outputted as a netCDF4 file in observation space. \n",
    "\n",
    "The model currently uses these three datasets made by CREDIT HERE, and available upon request from ACCESS HERE. These datasets include dynamic thickness anomaly data from balance years 1995 to 2021: `Dynamic_h_Greenland_GEMB_1994_2021.nc`, `Dynamic_h_Greenland_GSFC_1994_2021.nc`, and `Dynamic_h_Greenland_IMAU_1994_2020.nc`. See OBSERVATION DATA DOCUMENTATION HERE for more information about these datasets.\n",
    "\n",
    "### Multiple Comparisons\n",
    "This tool uses parallel processing to allow the user to efficiently compare multiple model files with multiple observation files. A comparison can occur between any observation file (of the three listed above) and any model file listed in the `model_fns` input in any year for which both have data. The user may request any amount of comparisons be done. See the \"Desired Comparisons\" subsection of the \"Input Data Requirements\" section for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db370cb2-bdfb-48f6-8282-aea2bc2ca4e5",
   "metadata": {},
   "source": [
    "## Input Data Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c5169-6df2-4ee9-a82f-a892e0024659",
   "metadata": {},
   "source": [
    "### Input netCDF file requirements\n",
    "The user may include multiple netCDF4 model files, but they must contain these four variables: x, y, time, and dh. \n",
    "\n",
    "#### Coordinate System (x and y variables)\n",
    "Coordinates are defined on a rectangular X-Y grid in the ISMIP6 standard projected polar-stereographic space. The ISMIP6 standard projection is defined [here](https://theghub.org/groups/ismip6/wiki/ISMIP6-Projections-Greenland). This means that in the model netCDF4 file, there must be variables x and y such that x has dimensions (x,) and length A and y has dimensions (y,) and length B. If multiple netCDF4 files are entered as inputs, their x and y variables are assumed to be the same, and behavior of this tool is undefined in the case that they are not. \n",
    "\n",
    "#### Time\n",
    "Must be an array of integers, floats, or strings representing the number of years since AD 0. \"1996\", 1996.0, and 1996 are all acceptable entries, but no month, day, time-of-day, or timezone information should be provided. If multiple netCDF4 files are entered as inputs, their time variables may be different (i.e. one model file may include data from 1996 to 2006, another may include data from 2000 to 2010) as long as they follow these guidelines.\n",
    "\n",
    "#### Dh variable\n",
    "The CmCt Grace Mascon tool expects the uploaded model to contain dynamic thickness data (the `dh` variable) for the comparison (should be in units of meters). dh must have dimensions (time, y, x) such that dh\\[i,j,k] is the change in dynamic thickness anomaly in meters from Sep 1st, time\\[i] to Aug 31st, time\\[i] + 1 at the position described by (x\\[k], y\\[j]) in ISMIP6 polar-stereographic space.\n",
    "\n",
    "\n",
    "### Desired Comparisons\n",
    "The filenames of all model files the user wishes to include should be listed in the variable `model_fns` as a list of strings. The input `desired_comparisons` is a list of tuples of the form `(obs_src, idx_mod, year)`. `obs_src` should be either \"IMAU\", \"GEMB\" or \"GSFC\" to indicate the observation dataset. `idx_mod` is an index of `model_fns` to indicate the model dataset. `year` should be a string of the form \"YYYY\" to indicate the year of this comparison. For each tuple in `desired_comparisons`, this tool will interpolate the model data from `model_fns[idx_mod]` in `year` to observation space and then subtract it from the `obs_src` data in `year`.\n",
    "\n",
    "Note that, for any comparison, `year` represents the desired balance year for the comparison. If `year` = 1996, then the observation change in dynamic thickness anomaly from Sep 1st, 1996 to Aug 31st 1997 will be compared with the modelled change in dynamic thickness anomaly from Sep 1st, 1996 to Aug 31st 1997. `year` cannot occur before the start of observation data collection (1995) or after the end of observation data collection (2021 for GSFC and GEMB, 2020 for IMAU). Likewise, `year` must correspond to an entry in the time variable of the model file.\n",
    "\n",
    "See the Inputs section for more inputs such as filenames and plotting preferences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d8f415-ed82-4b0c-865e-09ecb6c7294e",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d626ff-7423-4a58-99e1-6453cafa2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model filenames (see \"Desired Comparisons\" subsection above for more info)\n",
    "model_fns = [\"model_files/partial_model_file.nc\", \"model_files/model_file.nc\", \"model_files/doesnt_exist.nc\"]\n",
    "\n",
    "# Desired comparisons (see \"Desired Comparisons\" subsection above for more info)\n",
    "desired_comparisons = [(\"IMAU\", 0, \"2008\"),\n",
    "                       (\"GEMB\", 1, \"2010\"),\n",
    "                       (\"GEMB\", 0, \"2010\"), \n",
    "                       (\"IMAU\", 0, \"2010\"), \n",
    "                       (\"IMAU\", 1, \"2009\"),\n",
    "                       (\"GEMB\", 1, \"2011\"),\n",
    "                       (\"GEMB\", 0, \"2011\"), \n",
    "                       (\"IMAU\", 0, \"2011\")]\n",
    "\n",
    "# Output filename(s) in which to store residuals\n",
    "# The length of output_fns must be equal to the length of desired_comparisons if save_nc = True\n",
    "save_nc = True\n",
    "output_fns = [\"output_files/comp1.nc\",\n",
    "              \"output_files/comp2.nc\",\n",
    "              \"output_files/comp3.nc\",\n",
    "              \"output_files/comp4.nc\", \n",
    "              \"output_files/comp5.nc\",\n",
    "              \"output_files/comp6.nc\",\n",
    "              \"output_files/comp7.nc\",\n",
    "              \"output_files/comp8.nc\"]\n",
    "\n",
    "# Plotting preferences\n",
    "plot = False\n",
    "which_comparison_to_plot = 0   # Index in desired_comparisons of the comparison to plot\n",
    "save_plot = False   # Only relevant if plot = True\n",
    "plot_fn = \"output_files/plot.png\"   # Only relevant if plot = True and save_plot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a6707-3296-4487-916e-f22fb171aeaa",
   "metadata": {},
   "source": [
    "## Check Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acae3e18-c4ae-4ed5-ba58-545a443b645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "global error\n",
    "error = False\n",
    "\n",
    "if save_nc:\n",
    "    # Check that all output filenames are unique\n",
    "    if len(np.unique(output_fns)) < len(output_fns):\n",
    "        print(\"Error: At least two paths in output_fns are identical\")\n",
    "        error = True\n",
    "\n",
    "    # Check that there is one filename for each comparison\n",
    "    if not (len(output_fns) == len(desired_comparisons)):\n",
    "        print(f\"Error: {len(desired_comparisons)} comparisons requested but {len(output_fns)} output filenames provided\")\n",
    "        error = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f5342c-521c-4aa6-bb65-23d0e2faab73",
   "metadata": {},
   "source": [
    "## Read in Observation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b02ff9-bac2-4699-9258-f6648788275b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_obs_file(desired_comparisons):\n",
    "    global x_UTM\n",
    "    global y_UTM\n",
    "    global crs_wkt\n",
    "\n",
    "    # Outputs. For a more complete description, see the runProcessing function\n",
    "    all_dh_obs = []      # list of 2D arrays\n",
    "    comp_2_obs_idx = np.empty((len(desired_comparisons),), dtype = \"int\")\n",
    "\n",
    "    obs_paths = {\"IMAU\": \"observation_data/Dynamic_h_Greenland_IMAU_1994_2020.nc\", \n",
    "                 \"GEMB\": \"observation_data/Dynamic_h_Greenland_GEMB_1994_2021.nc\",\n",
    "                 \"GSFC\": \"observation_data/Dynamic_h_Greenland_GSFC_1994_2021.nc\"}\n",
    "\n",
    "    years_needed = {\"IMAU\": [], \"GEMB\":[], \"GSFC\":[]}   # Key: Source, Value: years that are needed for some comparison\n",
    "    comp_needing_year = {\"IMAU\":{}, \"GEMB\":{}, \"GSFC\":{}}   # Key: Source, Value: (Dictionary with key: year, value: index in desired_comparisons)\n",
    "\n",
    "    # Find out which of the files need to be read for the comparisons, and which years of those files need to be read\n",
    "    for i, comp in enumerate(desired_comparisons):\n",
    "        obs_src, _, year = comp\n",
    "        (years_needed[obs_src]).append(year)\n",
    "        (comp_needing_year[obs_src]).setdefault(year, [])\n",
    "        (comp_needing_year[obs_src])[year].append(i)\n",
    "    for src in years_needed:      # Convert to a set to eliminate duplicates\n",
    "        list = years_needed[src]\n",
    "        years_needed[src] = set(list)\n",
    "\n",
    "    set_common_vars = False\n",
    "    for src in [\"IMAU\", \"GEMB\", \"GSFC\"]:\n",
    "        if len(years_needed[src]) == 0:    # No comparison called for this file to be read, so skip it\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Read in file data\n",
    "        fn = obs_paths[src]\n",
    "        with h5py.File(fn, mode='r') as f:\n",
    "            if not set_common_vars:        # Ensures this is only done once, not three times\n",
    "                x_UTM = f['x'][:]\n",
    "                y_UTM = f['y'][:]\n",
    "                spatial_ref = f['spatial_ref']\n",
    "                crs_wkt = spatial_ref.attrs['crs_wkt'].decode('UTF-8')   # Contains coordinate system data\n",
    "                set_common_vars = True\n",
    "\n",
    "            time_obs = (f['time'][:]).astype(\"str\")\n",
    "            dh = f['dh'][:]    # Has shape (len(time_obs), len(y_UTM), len(x_UTM))\n",
    "\n",
    "\n",
    "        # Select only years of dh data needed for some comparison; construct outputs\n",
    "        for year in years_needed[src]:\n",
    "            # Check desired comparison year is not outside of bounds of observation data\n",
    "            year_dt = datetime.strptime(year, \"%Y\")\n",
    "            if datetime.strptime(time_obs[0], \"%Y\") > year_dt:\n",
    "                print(f\"Error: Desired comparison year ({year}) occurs before first date of observation data ({time_obs[0]})\")\n",
    "                return None, None\n",
    "            if datetime.strptime(time_obs[-1], \"%Y\") < year_dt:\n",
    "                print(f\"Error: Desired comparison year ({year}) occurs after last date of observation data ({time_obs[-1]})\")\n",
    "                return None, None\n",
    "\n",
    "            # Find the observation dh for the year specified\n",
    "            time_idx = np.argwhere(time_obs == year)[0,0]\n",
    "            dh_obs = dh[time_idx, :, ]\n",
    "\n",
    "            # Append to all_dh_obs and update comp_2_obs_idx\n",
    "            all_dh_obs.append(dh_obs)\n",
    "            obs_idx = len(all_dh_obs) - 1\n",
    "            for i in (comp_needing_year[src])[year]:\n",
    "                comp_2_obs_idx[i] = obs_idx\n",
    "                 \n",
    "        dh = None   # De-allocate memory\n",
    "        \n",
    "    return all_dh_obs, comp_2_obs_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c3ac4-ee89-405e-9a18-62760b58f599",
   "metadata": {},
   "source": [
    "## Read in Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ce23e37-a282-4f3d-ba64-4e1f107bacd8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_model_file(model_fns, desired_comparisons):\n",
    "    global x_mod\n",
    "    global y_mod\n",
    "\n",
    "    # Outputs. For a more complete description, see the runProcessing function\n",
    "    all_dh_mod = []\n",
    "    comp_2_mod_idx = np.empty((len(desired_comparisons),), dtype = \"int\")\n",
    "\n",
    "    years_needed = {}        # Key: index in model_fns, Value: List of years to be read\n",
    "    comp_needing_year = {}   # Key: index in model_fns: Value: (Dictionary with key: year and value: index in desired_comparisons)\n",
    "\n",
    "    # Find out which of the files need to be read for the comparisons\n",
    "    for i, comp in enumerate(desired_comparisons):\n",
    "        _, idx_mod, year = comp\n",
    "        years_needed.setdefault(idx_mod, [])\n",
    "        (years_needed[idx_mod]).append(year)\n",
    "        comp_needing_year.setdefault(idx_mod, {})\n",
    "        (comp_needing_year[idx_mod]).setdefault(year, [])\n",
    "        (comp_needing_year[idx_mod])[year].append(i)\n",
    "    for src in years_needed:      # Convert to a set to eliminate duplicates\n",
    "        list = years_needed[src]\n",
    "        years_needed[src] = set(list)\n",
    "\n",
    "    \n",
    "    set_common_vars = False\n",
    "    for idx_mod in years_needed:     # Iterate over model files\n",
    "\n",
    "        # Read file data\n",
    "        fn = model_fns[idx_mod]\n",
    "        with h5py.File(fn , mode='r') as f:\n",
    "            if not set_common_vars:\n",
    "                # See \"Input Data Requirements\" section for more details on the format of these variables\n",
    "                x_mod = f['x'][:]\n",
    "                y_mod = f['y'][:]\n",
    "                set_common_vars = True\n",
    "\n",
    "            time_mod = f['time'][:]\n",
    "            time_mod = np.array([str(int(t)) for t in time_mod])\n",
    "            dh = f['dh'][:]\n",
    "\n",
    "\n",
    "        # Select only years of dh data needed for some comparison; construct outputs\n",
    "        for year in years_needed[idx_mod]:\n",
    "            # Check desired comparison year is not outside of bounds of model data\n",
    "            year_dt = datetime.strptime(year, \"%Y\")\n",
    "            if datetime.strptime(time_mod[0], \"%Y\") > year_dt:\n",
    "                print(f\"Error: Desired comparison year ({year}) occurs before first date of model data ({time_mod[0]})\")\n",
    "                return None, None\n",
    "            if datetime.strptime(time_mod[-1], \"%Y\") < year_dt:\n",
    "                print(f\"Error: Desired comparison year ({year}) occurs after last date of model data ({time_mod[-1]})\")\n",
    "                return None, None\n",
    "\n",
    "            # Find the dh in the year specified\n",
    "            time_idx = np.argwhere(time_mod == year)[0,0]\n",
    "            try:\n",
    "                dh_mod = dh[time_idx, :, :]\n",
    "            except:\n",
    "                print(f\"Error: Could not find year {year} in model file {fn}\")\n",
    "                return None, None\n",
    "\n",
    "            # Append to all_dh_mod and update comp_2_mod_idx\n",
    "            all_dh_mod.append(dh_mod)\n",
    "            mod_idx = len(all_dh_mod) - 1\n",
    "            for i in (comp_needing_year[idx_mod])[year]:\n",
    "                comp_2_mod_idx[i] = mod_idx\n",
    "                \n",
    "        dh = None   # De-allocate memory\n",
    "            \n",
    "    return all_dh_mod, comp_2_mod_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42aba1-dfda-4bf7-a1a4-142baee9e90b",
   "metadata": {},
   "source": [
    "## Transform Observation Coordinates from UTM-24N to ISMIP6 polar-stereographic coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbcb88af-e681-44de-90bb-7aa284b3ec55",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def transform_obs_coords(crs_wkt, x_UTM, y_UTM):\n",
    "    # See \"About This Tool\" section for observation coordinate system information\n",
    "    \n",
    "    # Make a transformer from observation coordinates to model coordinates\n",
    "    crs_utm = pyproj.crs.CRS.from_wkt(crs_wkt)    # Coordinate system of observation data\n",
    "    crs_ps = pyproj.crs.CRS(ccrs.Stereographic(central_latitude= 90.0, central_longitude= -45.0, false_easting= 0.0, \n",
    "                            false_northing= 0.0, true_scale_latitude= 70.0, globe=ccrs.Globe('WGS84')))    # ISMIP6 coord sys\n",
    "    utm_to_ps = pyproj.Transformer.from_crs(crs_from = crs_utm, crs_to = crs_ps)\n",
    "\n",
    "    # Format observation coordinate data into two 1D arrays of the same length\n",
    "    xv, yv = np.meshgrid(x_UTM, y_UTM)\n",
    "    x_UTM_points, y_UTM_points = xv.transpose().flatten(), yv.transpose().flatten()\n",
    "\n",
    "    # Transform observation coordinate points to points in polar-stereographic space\n",
    "    x_obs, y_obs = utm_to_ps.transform(x_UTM_points, y_UTM_points)\n",
    "    return x_obs, y_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a683ef-0633-4acd-8ba4-302c63f02c6e",
   "metadata": {},
   "source": [
    "## Define Bilinear Interpolation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614838df-8eea-4200-b263-e883a1580f00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def bilinear_interp(x, y, arr, x_query, y_query):\n",
    "    \"\"\"\n",
    "    Bilinearly interpolates the value of arr, defined on a rectangular grid by x and y, at the set of points defined \n",
    "    by x_query and y_query. x, y, x_query, and y_query must all be in the same coordinate system.\n",
    "\n",
    "    Parameters:\n",
    "    arr: An array of floats of shape (ny, nx)\n",
    "    x, y: x has shape (nx,) and y has shape (ny,). The entry arr[i][j] corresponds to the point in space defined by \n",
    "    y[i] and x[j]. The entries of x and y must be evenly spaced\n",
    "    x_query, y_query: Both must have shape (n,), where n is the number of query points. It is assumed that x, y, \n",
    "    x_query, and y_query are defined in the same coordinate system and that x and x_query represent the same dimension, \n",
    "    while y and y_query represent the other dimension \n",
    "\n",
    "    Returns: \n",
    "    arr_query: An array of floats of shape (n,). arr_query[i] represents the estimated value of arr at the position \n",
    "    defined by x_query[i] and y_query[i], estimated using bilinear interpolation. If any query point i is outside the \n",
    "    bounds defined by x and y, arr_query[i] will be NaN\n",
    "    \"\"\"\n",
    "\n",
    "    # Query points\n",
    "    x_query = np.asarray(x_query)\n",
    "    y_query = np.asarray(y_query)\n",
    "\n",
    "    # Find the spacing of the x and y arrays\n",
    "    sx, sy = x[1] - x[0], y[1] - y[0]\n",
    "    a = sx * sy  # area of each grid cell\n",
    "\n",
    "    # Find the indices of the four closest points in arr to each query point\n",
    "    x0 = np.floor((x_query - x[0]) / sx).astype(int)   # Left\n",
    "    x1 = x0 + 1                                        # Right\n",
    "    y0 = np.floor((y_query - y[0]) / sy).astype(int)   # Bottom\n",
    "    y1 = y0 + 1                                        # Top\n",
    "\n",
    "    # ma[i] is True if both x_query[i] and y_query[i] are within the bounds of the grid defined by x and y\n",
    "    ma = (x0 >= 0) & (x1 < len(x)) & (y0 >= 0) & (y1 < len(y))\n",
    "\n",
    "    # Values at each of the reference coordinates\n",
    "    Ia, Ib, Ic, Id = (np.empty_like(x_query, dtype=np.float64), np.empty_like(x_query, dtype=np.float64),\n",
    "                     np.empty_like(x_query, dtype=np.float64), np.empty_like(x_query, dtype=np.float64))  # Allocate memory\n",
    "    Ia[~ma], Ib[~ma], Ic[~ma], Id[~ma] = np.nan, np.nan, np.nan, np.nan    # Set to NaN where x or y are out of bounds\n",
    "    Ia[ma] = arr[ y0[ma], x0[ma] ]    # Bottom left\n",
    "    Ib[ma] = arr[ y1[ma], x0[ma] ]    # Top left\n",
    "    Ic[ma] = arr[ y0[ma], x1[ma] ]    # Bottom right\n",
    "    Id[ma] = arr[ y1[ma], x1[ma] ]    # Top right\n",
    "\n",
    "    # Weights for each reference coordinate\n",
    "    wa, wb, wc, wd = (np.empty_like(x_query, dtype=np.float64), np.empty_like(x_query, dtype=np.float64),\n",
    "                     np.empty_like(x_query, dtype=np.float64), np.empty_like(x_query, dtype=np.float64))  # Allocate memory\n",
    "    wa[~ma], wb[~ma], wc[~ma], wd[~ma] = np.nan, np.nan, np.nan, np.nan    # Set to NaN where x or y are out of bounds\n",
    "    wa[ma] = ((x[x1[ma]] - x_query[ma]) * (y[y1[ma]] - y_query[ma])) / a\n",
    "    wb[ma] = ((x[x1[ma]] - x_query[ma]) * (y_query[ma] - y[y0[ma]])) / a\n",
    "    wc[ma] = ((x_query[ma] - x[x0[ma]]) * (y[y1[ma]] - y_query[ma])) / a\n",
    "    wd[ma] = ((x_query[ma] - x[x0[ma]]) * (y_query[ma] - y[y0[ma]])) / a\n",
    "\n",
    "    return wa*Ia + wb*Ib + wc*Ic + wd*Id     # arr_query in documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8380b6-e809-4d59-ad62-8ed44f94ff8d",
   "metadata": {},
   "source": [
    "## Interpolate Model to Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201893d4-6347-4133-b2c3-5edc873b7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_one_model_to_obs_space(dh_mod):\n",
    "    \"\"\"\n",
    "    Interpolates a model dh 2D array (representing a single balance year) to observation space and \n",
    "    reshape to newshape\n",
    "    \"\"\"\n",
    "    \n",
    "    # Bilinearly interpolate model to observation space\n",
    "    model_dh_interped_flat = bilinear_interp(x_mod, y_mod, dh_mod, x_query, y_query)\n",
    "\n",
    "    # Reshape model array to the same shape as the observation array\n",
    "    dh_mod_interped = np.empty_like(I_, dtype = np.float64)\n",
    "    dh_mod_interped[~I_] = np.nan     # Fill with NaN at indices where dh_obs is NaN\n",
    "    dh_mod_interped[I_] = model_dh_interped_flat\n",
    "    dh_mod_interped = np.reshape(dh_mod_interped, newshape, order = \"F\")\n",
    "    return dh_mod_interped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12130870-606f-43ac-9c5f-80e3c9c2db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_all_models_to_observation_space(x_obs, y_obs, dh_obs, all_dh_mod):\n",
    "    \"\"\"\n",
    "    Wrapper function for interp_one_model_to_obs_space which parallelizes the interpolation operation of all models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set inputs to interp_one_model_to_obs_space that will not vary between calls\n",
    "    global I_\n",
    "    global x_query, y_query     # Interpolation query points (does not include points which are NaN in observation data)\n",
    "    global newshape             # Shape that the model will be interpolated to\n",
    "    \n",
    "    I_ = (~np.isnan(dh_obs)).transpose().flatten()   # I_[i] is False if x_obs[i], y_obs[i] correspond to a point where dh_obs is NaN, True otherwise\n",
    "    x_query, y_query = x_obs[I_], y_obs[I_]\n",
    "    newshape = dh_obs.shape\n",
    "\n",
    "    # Parallelize the interpolation and reshaping step in order to speed up\n",
    "    if len(all_dh_mod) == 1:\n",
    "        return [interp_one_model_to_obs_space(all_dh_mod[0])]\n",
    "    else:\n",
    "        all_dh_mod_interped = []\n",
    "        inputs = []\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            generator = executor.map(interp_one_model_to_obs_space, all_dh_mod)\n",
    "        for dh_mod_interped in generator:\n",
    "            all_dh_mod_interped.append(dh_mod_interped)\n",
    "        return all_dh_mod_interped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f06a830-13cd-457f-9c76-e9b38833f5dc",
   "metadata": {},
   "source": [
    "## Subtract Model and Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe33180-1a35-468d-ab84-9eaada790a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals(all_dh_obs, comp_2_obs_idx, all_dh_mod_interped, comp_2_mod_idx):\n",
    "    \"\"\"\n",
    "    For each comparison in desired_comparisons, computes the residual and outputs all residuals\n",
    "    as a list such that all_dh_res[i] corresponds to desired_comparison[i]\n",
    "\n",
    "    See runProcessing function for details on the inputs to this function\n",
    "    \"\"\"\n",
    "    all_dh_res = []\n",
    "    for i in range(len(comp_2_obs_idx)):\n",
    "        obs_idx = comp_2_obs_idx[i]\n",
    "        mod_idx = comp_2_mod_idx[i]\n",
    "        all_dh_res.append(all_dh_mod_interped[mod_idx] - all_dh_obs[obs_idx])  # residual = model - observation\n",
    "    return all_dh_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e16ab-a3ec-40a1-89f4-06820109886c",
   "metadata": {},
   "source": [
    "## Plot Observation, Model, and Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac3a4f9-aae5-4b30-bd1c-e4b886aaafe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_obs_mod_res(extent, dh_obs, dh_mod_interped, dh_res, year, save_plot):\n",
    "    mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "    mpl.rcParams['figure.dpi'] = 100\n",
    "    fig = plt.figure(figsize=(24,14))\n",
    "\n",
    "    \"\"\"\n",
    "    crs = ccrs.UTM('24')\n",
    "    coords = {\"x\": x_UTM, \"y\": y_UTM}\n",
    "    dims = [\"y\", \"x\"]\n",
    "    \n",
    "    # Observed\n",
    "    ax1 = plt.subplot(projection = crs)\n",
    "    #ax1 = plt.subplot(131)\n",
    "    #ax1.set_extent(extent) # Map bounds, [west, east, south, north]\n",
    "    xr_obs = xr.DataArray(dh_obs, coords = coords, dims = dims)\n",
    "    xr.plot.imshow(xr_obs, ax=ax1, transform = crs) \n",
    "    ax1.set_title(\"dh_obs\")\n",
    "    \"\"\"\n",
    "    mins = [np.min(dh_obs), np.min(dh_mod_interped), np.min(dh_res)]\n",
    "    min = np.min(mins)\n",
    "    maxs = [np.max(dh_obs), np.max(dh_mod_interped), np.max(dh_res)]\n",
    "    max = np.max(maxs)\n",
    "    \n",
    "\n",
    "    ax1 = plt.subplot(131)\n",
    "    im1 = ax1.imshow(dh_obs, aspect = \"equal\", origin = \"lower\", extent = extent, vmin = min, vmax = max, cmap = \"RdYlBu\")\n",
    "    ax1.set_xbound(lower = extent[0], upper = extent[1])\n",
    "    ax1.set_ybound(lower = extent[2], upper = extent[3])\n",
    "    ax1.set_xlim(left = extent[0], right = extent[1])\n",
    "    ax1.set_ylim(bottom = extent[2], top = extent[3])\n",
    "    ax1.set_title(f\"Observed Dynamic Thickness for {year}\", fontsize = 18)\n",
    "    \n",
    "    \n",
    "    # Model\n",
    "    ax2 = plt.subplot(132)\n",
    "    im2 = ax2.imshow(dh_mod_interped, aspect = \"equal\", origin = \"lower\", extent = extent, vmin = min, vmax = max, cmap = \"RdYlBu\")\n",
    "    ax2.set_xbound(lower = extent[0], upper = extent[1])\n",
    "    ax2.set_ybound(lower = extent[2], upper = extent[3])\n",
    "    ax2.set_xlim(left = extent[0], right = extent[1])\n",
    "    ax2.set_ylim(bottom = extent[2], top = extent[3])\n",
    "    ax2.set_title(f\"Modelled Dynamic Thickness for {year} (interpolated)\", fontsize = 18)\n",
    "    \n",
    "\n",
    "    # Residual\n",
    "    ax3 = plt.subplot(133)\n",
    "    im3 = ax3.imshow(dh_res, aspect = \"equal\", origin = \"lower\", extent = extent, vmin = min, vmax = max, cmap = \"RdYlBu\")\n",
    "    ax3.set_xbound(lower = extent[0], upper = extent[1])\n",
    "    ax3.set_ybound(lower = extent[2], upper = extent[3])\n",
    "    ax3.set_xlim(left = extent[0], right = extent[1])\n",
    "    ax3.set_ylim(bottom = extent[2], top = extent[3])\n",
    "    ax3.set_title(f\"Residual Dynamic Thickness for {year}\", fontsize = 18)\n",
    "    \n",
    "    \n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.875, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(im1, cax=cbar_ax)\n",
    "    fig.colorbar(im2, cax=cbar_ax)\n",
    "    fig.colorbar(im3, cax=cbar_ax)\n",
    "    cbar_ax.set_ylabel(\"meters\", fontsize = 16)\n",
    "\n",
    "    fig.suptitle(\"Dynamic Thickness Anomaly Change\", fontsize = 24)\n",
    "    \n",
    "    if save_plot:\n",
    "        plt.savefig(plot_fn)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebbded2-6759-498d-91b9-b42088afea02",
   "metadata": {},
   "source": [
    "## Save to netCDF File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3c44821-956c-4128-80d7-01b20086147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_one_residual_to_netcdf(tuple):\n",
    "    \"\"\"\n",
    "    Saves the residual of a single comparison to a netcdf file\n",
    "    \"\"\"\n",
    "    fn, dh_res, year = tuple\n",
    "    \n",
    "    rootgrp = netCDF4.Dataset(fn, \"w\", format=\"NETCDF4\")\n",
    "    \n",
    "    # Record coordinate system\n",
    "    spatial_ref_var = rootgrp.createVariable(\"spatial_ref\", \"i8\")\n",
    "    spatial_ref_var[:] = 0\n",
    "    spatial_ref_var.crs_wkt = crs_wkt\n",
    "    \n",
    "    # Set up x and y variables\n",
    "    x_dim = rootgrp.createDimension(\"x\", len(x_UTM))\n",
    "    y_dim = rootgrp.createDimension(\"y\", len(y_UTM))\n",
    "    x_var = rootgrp.createVariable(\"x\", \"f4\", (\"x\",))\n",
    "    y_var = rootgrp.createVariable(\"y\", \"f4\", (\"y\",))\n",
    "    x_var[:], y_var[:] = x_UTM, y_UTM\n",
    "    x_var.units, y_var.units = \"meter\", \"meter\"\n",
    "    \n",
    "    # Set up dynamic thickness residual variable\n",
    "    dh_res_var = rootgrp.createVariable(\"dh_res\", \"f4\", (\"y\",\"x\",))\n",
    "    dh_res_var[:,:] = dh_res\n",
    "    dh_res_var.coordinates = \"spatial_ref\"\n",
    "    dh_res_var.grid_mapping = \"spatial_ref\"\n",
    "    dh_res_var.long_name = \"Residual of annual dynamic mass/ice thickness (assume ice density of 917 kg/m3) change between Sep 1 of \" + year + \" and \" + str(int(year) + 1)\n",
    "    dh_res_var.units = \"meters\"\n",
    "    rootgrp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2e66ba-2dce-4c8f-a314-9ab7ca7a2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_residuals_to_netcdf(output_fns, all_dh_res, x_UTM, y_UTM, desired_comparisons, crs_wkt):\n",
    "    \"\"\"\n",
    "    Wrapper function for save_one_residual_to_netcdf which parallelizes the saving of all residuals\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(all_dh_res) == 1:\n",
    "        save_one_residual_to_netcdf(output_fns[0], all_dh_res[0], (desired_comparisons[0])[2])\n",
    "    else:\n",
    "        inputs = []\n",
    "        for i in range(len(desired_comparisons)):\n",
    "            inputs.append((output_fns[i], all_dh_res[i], (desired_comparisons[i])[2]))\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            results = executor.map(save_one_residual_to_netcdf, inputs)\n",
    "            for result in results:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33032177-3995-456f-895b-f8625e66e69c",
   "metadata": {},
   "source": [
    "## Compute Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d581b2db-e0a1-46ee-a164-2495188efef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runProcessing():\n",
    "    \"\"\"\n",
    "    Before reading, see the \"Desired Comparisons\" subsection of the \"Input Data Requirements\" section.\n",
    "\n",
    "    Here is a summary of what the tool does at a basic level. First it reads in the model and \n",
    "    observation data for the appropriate years. It then transforms the spatial coordinates of the \n",
    "    observation data from a grid in UTM-24N to a set of non-gridded points in polar-stereographic\n",
    "    coordinates. Then, using the fact that the model data is a grid in polar-stereographic, the tool\n",
    "    uses bilinear interpolation to estimate the value of the model data at the locations of the \n",
    "    observation data. This and the observation data are then subtracted, giving the residual. Below \n",
    "    is a more detailed account of the pipeline, which defines the behavior of important variables.\n",
    "\n",
    "    Pipeline:\n",
    "    1) Read in observation data as a list of 2D arrays called all_dh_obs. Each entry of all_dh_obs has\n",
    "       the same shape, let this be (ny, nx). Each entry of all_dh_obs is for a different year or source \n",
    "       necessary in some comparison in desired_comparisons. The variable comp_2_obs_idx records which \n",
    "       comparisons need which entry of all_dh_obs: comp_2_obs_idx[i] is the index of all_dh_obs which \n",
    "       holds the observation data needed for desired_comparison[i]. The spatial information for all \n",
    "       observation data arrays is stored in x_UTM and y_UTM, and x_UTM has length nx and y_UTM has \n",
    "       length ny.\n",
    "    2) Read in model data as a list of 2D arrays called all_dh_mod. Each entry of all_dh_mod is assumed \n",
    "       to have the same shape. Each entry of all_dh_mod is for a different year or model necessary in some \n",
    "       comparison in desired_comparisons. The variable comp_2_mod_idx records which comparisons need \n",
    "       which entry of all_dh_mod: comp_2_mod_idx[i] is the index of all_dh_mod which holds the model \n",
    "       data needed for desired_comparison[i]. The spatial information for all model data arrays is stored \n",
    "       in x_mod and y_mod. See the \"Coordinate System\" subsection of the \"Input Data Requirements\" section \n",
    "       for more information.\n",
    "    3) Reshape coordinate information encoded in x_UTM and y_UTM to a set of points described by two 1D \n",
    "       arrays (both with length nx * ny), which are then converted into polar stereographic coordinates. \n",
    "       These arrays are x_obs and y_obs.\n",
    "    4) For each entry of all_dh_mod, bilinearly interpolate from the model's grid in polar stereographic, \n",
    "       using x_obs and y_obs as the query points. This will return an estimated value of the modelled \n",
    "       value at each of the points for which the observation value is known. In other words, the model has \n",
    "       been brought to observation space. Note that this step is done in parallel.\n",
    "    5) For each desired comparison, compute the residual (model - observation) in observation space\n",
    "    6) (Optional) Write all residuals to netCDF4 files\n",
    "    7) (Optional) For a single comparison, plot the observation, model, and residual and optionally save \n",
    "        the plot\n",
    "    \"\"\"\n",
    "    if error:\n",
    "        return None\n",
    "    \n",
    "    update_progress(0.0, \"Starting...              \", 0.0)\n",
    "    t = time.time()\n",
    "    all_dh_obs, comp_2_obs_idx = read_obs_file(desired_comparisons)     # Sets global variables x_UTM, y_UTM, crs_wkt\n",
    "    if all_dh_obs is None:\n",
    "        return None\n",
    "    update_progress(0.2, \"Read Observation Data    \", time.time() - t)\n",
    "\n",
    "    \n",
    "    t = time.time()\n",
    "    all_dh_mod, comp_2_mod_idx = read_model_file(model_fns, desired_comparisons)     # Sets global variables x_mod, y_mod\n",
    "    if all_dh_mod is None:\n",
    "        return None\n",
    "    update_progress(0.4, \"Read Model Data          \", time.time() - t)\n",
    "\n",
    "    \n",
    "    t = time.time()\n",
    "    x_obs, y_obs,  = transform_obs_coords(crs_wkt, x_UTM, y_UTM)\n",
    "    update_progress(0.5, \"Transformed Coordinates  \", time.time() - t)\n",
    "\n",
    "    \n",
    "    t = time.time()\n",
    "    all_dh_mod_interped = interpolate_all_models_to_observation_space(x_obs, y_obs, all_dh_obs[0], all_dh_mod)\n",
    "    all_dh_mod = None    # Deallocate memory\n",
    "    update_progress(0.7, \"Interpolated Model       \", time.time() - t)\n",
    "\n",
    "    t = time.time()\n",
    "    all_dh_res = compute_residuals(all_dh_obs, comp_2_obs_idx, all_dh_mod_interped, comp_2_mod_idx)\n",
    "    update_progress(0.8, \"Computed Residuals       \", time.time() - t)\n",
    "    \n",
    "\n",
    "    if save_nc:\n",
    "        t = time.time()\n",
    "        save_residuals_to_netcdf(output_fns, all_dh_res, x_UTM, y_UTM, desired_comparisons, crs_wkt)\n",
    "        update_progress(0.9, \"Saved residuals to NetCDF\", time.time() - t)\n",
    "    \n",
    "\n",
    "    if plot:\n",
    "        print(\"Plotting....\")\n",
    "        t = time.time()\n",
    "        i = which_comparison_to_plot\n",
    "        plot_obs_mod_res((x_UTM[0], x_UTM[-1], y_UTM[0], y_UTM[-1]), all_dh_obs[comp_2_obs_idx[i]], \n",
    "                         all_dh_mod_interped[comp_2_mod_idx[i]], all_dh_res[i], (desired_comparisons[i])[2], save_plot)\n",
    "        update_progress(1.0, \"Plotted Results          \", time.time() - t)\n",
    "    \n",
    "    print(\"Success\")\n",
    "    return \"Success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "873a6429-ba92-4806-9b53-c77283efca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update processing progress bar\n",
    "def update_progress(progress, title, time_elapsed_print):\n",
    "    bar_length = 20\n",
    "    block = int(20.0*progress)\n",
    "    text = title+\" [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100) + f\" ({time_elapsed_print} seconds)\"\n",
    "    print(text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c9d0d89-6964-46da-8316-816e1f99b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...               [--------------------] 0.0% (0.0 seconds)\n",
      "Read Observation Data     [####----------------] 20.0% (4.601094961166382 seconds)\n",
      "Read Model Data           [########------------] 40.0% (1.0195326805114746 seconds)\n",
      "Transformed Coordinates   [##########----------] 50.0% (1.405395269393921 seconds)\n",
      "Interpolated Model        [##############------] 70.0% (2.9890050888061523 seconds)\n",
      "Computed Residuals        [################----] 80.0% (0.21115922927856445 seconds)\n",
      "Saved residuals to NetCDF [##################--] 90.0% (1.3754267692565918 seconds)\n",
      "Success\n",
      "Total time: 11.695570468902588 seconds\n"
     ]
    }
   ],
   "source": [
    "t_tot = time.time()\n",
    "runProcessing()\n",
    "print(\"Total time: \" + str(time.time() - t_tot) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f7736-5cd2-438a-897d-bf6365c39911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
